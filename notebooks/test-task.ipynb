{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"},{"sourceId":7703548,"sourceType":"datasetVersion","datasetId":4497165}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport keras.backend as K\nimport tensorflow as tf\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:52:43.403867Z","iopub.execute_input":"2024-02-26T08:52:43.404281Z","iopub.status.idle":"2024-02-26T08:52:43.411174Z","shell.execute_reply.started":"2024-02-26T08:52:43.404251Z","shell.execute_reply":"2024-02-26T08:52:43.409990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.experimental.list_physical_devices('GPU')\n\nif gpus:\n    try:\n        # Set TensorFlow to only use the first GPU\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        # Visible devices must be set before GPUs have been initialized\n        print(e)\nelse:\n    print(\"No GPUs found!\")","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:07.613896Z","iopub.execute_input":"2024-02-26T08:53:07.614297Z","iopub.status.idle":"2024-02-26T08:53:07.621904Z","shell.execute_reply.started":"2024-02-26T08:53:07.614267Z","shell.execute_reply":"2024-02-26T08:53:07.620795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_path = '/kaggle/input/airbus-ship-detection'\ntrain_dir_path = os.path.join(input_path, 'train_v2/')\ntest_dir_path = os.path.join(input_path, 'test_v2/')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:09.858853Z","iopub.execute_input":"2024-02-26T08:53:09.859278Z","iopub.status.idle":"2024-02-26T08:53:09.865287Z","shell.execute_reply.started":"2024-02-26T08:53:09.859246Z","shell.execute_reply":"2024-02-26T08:53:09.864100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_masks = pd.read_csv(os.path.join(input_path,'train_ship_segmentations_v2.csv'))\ntrain_masks","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:10.064145Z","iopub.execute_input":"2024-02-26T08:53:10.064968Z","iopub.status.idle":"2024-02-26T08:53:11.526304Z","shell.execute_reply.started":"2024-02-26T08:53:10.064923Z","shell.execute_reply":"2024-02-26T08:53:11.525164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_masks.shape","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:11.528278Z","iopub.execute_input":"2024-02-26T08:53:11.528708Z","iopub.status.idle":"2024-02-26T08:53:11.536082Z","shell.execute_reply.started":"2024-02-26T08:53:11.528678Z","shell.execute_reply":"2024-02-26T08:53:11.534975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 231723 images","metadata":{}},{"cell_type":"code","source":"train_masks.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:11.537713Z","iopub.execute_input":"2024-02-26T08:53:11.538048Z","iopub.status.idle":"2024-02-26T08:53:11.601164Z","shell.execute_reply.started":"2024-02-26T08:53:11.538022Z","shell.execute_reply":"2024-02-26T08:53:11.599943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see, that there is no NaN or empty values in ImageIds, and we have 150000 empty maskencdoing. It means, that our data contains 150000 images with no ships in it, and 81723 with ships in images","metadata":{}},{"cell_type":"code","source":"train_masks['ImageId'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:11.603706Z","iopub.execute_input":"2024-02-26T08:53:11.604087Z","iopub.status.idle":"2024-02-26T08:53:11.865415Z","shell.execute_reply.started":"2024-02-26T08:53:11.604056Z","shell.execute_reply":"2024-02-26T08:53:11.864267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_masks['ImageId'].value_counts().shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:11.866991Z","iopub.execute_input":"2024-02-26T08:53:11.867355Z","iopub.status.idle":"2024-02-26T08:53:12.081320Z","shell.execute_reply.started":"2024-02-26T08:53:11.867325Z","shell.execute_reply":"2024-02-26T08:53:12.080194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see, that we have some duplicates in ImageIds, and have 192556 unique images.","metadata":{}},{"cell_type":"code","source":"train_masks[train_masks['ImageId'] == 'e6fd0c12e.jpg']","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:12.083508Z","iopub.execute_input":"2024-02-26T08:53:12.083878Z","iopub.status.idle":"2024-02-26T08:53:12.143494Z","shell.execute_reply.started":"2024-02-26T08:53:12.083847Z","shell.execute_reply":"2024-02-26T08:53:12.142139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_img(img_id):\n    img = Image.open(os.path.join(train_dir_path, img_id))\n    plt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:12.145188Z","iopub.execute_input":"2024-02-26T08:53:12.145593Z","iopub.status.idle":"2024-02-26T08:53:12.155235Z","shell.execute_reply.started":"2024-02-26T08:53:12.145562Z","shell.execute_reply":"2024-02-26T08:53:12.153948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_img('e6fd0c12e.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:12.157933Z","iopub.execute_input":"2024-02-26T08:53:12.158346Z","iopub.status.idle":"2024-02-26T08:53:12.685201Z","shell.execute_reply.started":"2024-02-26T08:53:12.158313Z","shell.execute_reply":"2024-02-26T08:53:12.683832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_decode(mask_rle, input_shape=(768,768)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    img=np.zeros(input_shape[0]*input_shape[1], dtype=np.float32)\n    if not(type(mask_rle) is float):\n        s = mask_rle.split()\n        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n        starts -= 1\n        ends = starts + lengths\n        for lo, hi in zip(starts, ends):\n            img[lo:hi] = 1.0\n    return img.reshape((input_shape[0],input_shape[1])).T\n\ndef show_decode(mask_rle, shape=(768, 768)):\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T  # Needed to align to RLE direction\n\ndef masks_as_image(in_mask_list):\n    all_masks = np.zeros((768, 768), dtype = np.int16)\n    for mask in in_mask_list:\n        if isinstance(mask, str):\n            all_masks += show_decode(mask)\n    return np.expand_dims(all_masks, -1)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:12.687496Z","iopub.execute_input":"2024-02-26T08:53:12.687909Z","iopub.status.idle":"2024-02-26T08:53:12.706624Z","shell.execute_reply.started":"2024-02-26T08:53:12.687874Z","shell.execute_reply":"2024-02-26T08:53:12.705316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_img = masks_as_image(train_masks.query('ImageId==\"e6fd0c12e.jpg\"')['EncodedPixels'])\nplt.imshow(mask_img[:, :, 0])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:13.031564Z","iopub.execute_input":"2024-02-26T08:53:13.032093Z","iopub.status.idle":"2024-02-26T08:53:13.305904Z","shell.execute_reply.started":"2024-02-26T08:53:13.032044Z","shell.execute_reply":"2024-02-26T08:53:13.305044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_img = masks_as_image(train_masks.query('ImageId==\"e6fd0c12e.jpg\"')['EncodedPixels'])\nplt.imshow(mask_img[:, :, 0])\nplt.xlim((600, 800))  \nplt.ylim((200, 500))  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:13.307504Z","iopub.execute_input":"2024-02-26T08:53:13.308021Z","iopub.status.idle":"2024-02-26T08:53:13.512435Z","shell.execute_reply.started":"2024-02-26T08:53:13.307990Z","shell.execute_reply":"2024-02-26T08:53:13.511616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can be now sure, that the duplicates imageids means, that we have more then one ship on image. And amount of ships on image equals amount of duplicates","metadata":{}},{"cell_type":"markdown","source":"So now, we can make feature like has ship, and then aggregate it with sum and become amount of ships in each image, and then we can plot distribution of it.","metadata":{}},{"cell_type":"markdown","source":"Also i will research image sizes, so we can see, if all images have normal size and there are or not some corupted files or something else, with ploting its distribution ","metadata":{}},{"cell_type":"code","source":"train_masks['has_ship'] = train_masks['EncodedPixels'].map(lambda x: 1 if x is not np.NaN else 0)\ntrain_masks","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:14.306985Z","iopub.execute_input":"2024-02-26T08:53:14.307647Z","iopub.status.idle":"2024-02-26T08:53:14.517723Z","shell.execute_reply.started":"2024-02-26T08:53:14.307607Z","shell.execute_reply":"2024-02-26T08:53:14.516427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_masks_aggregated = train_masks.groupby('ImageId').agg({'has_ship':'sum'}).reset_index()\ntrain_masks_aggregated = train_masks_aggregated.rename(columns={'has_ship': 'ship_amount'})\ntrain_masks_aggregated","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:14.519895Z","iopub.execute_input":"2024-02-26T08:53:14.520325Z","iopub.status.idle":"2024-02-26T08:53:14.822136Z","shell.execute_reply.started":"2024-02-26T08:53:14.520292Z","shell.execute_reply":"2024-02-26T08:53:14.821204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(train_masks_aggregated['ship_amount'], bins=10)  \nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.title('Distribution of ' + 'ship_amount')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:14.824044Z","iopub.execute_input":"2024-02-26T08:53:14.824761Z","iopub.status.idle":"2024-02-26T08:53:15.093310Z","shell.execute_reply.started":"2024-02-26T08:53:14.824721Z","shell.execute_reply":"2024-02-26T08:53:15.092279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can observe, that most of our images doesn't contain any ships and are empty, so we have some class imbalance, which i need to handle before training process.","metadata":{}},{"cell_type":"markdown","source":"Now, let's drop zero values, to see distribution of ships amount on non-empty images","metadata":{}},{"cell_type":"code","source":"plt.hist(train_masks_aggregated[train_masks_aggregated['ship_amount']>0]['ship_amount'], bins=15)  \nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.title('Distribution of ship_amount without zeros')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:15.367214Z","iopub.execute_input":"2024-02-26T08:53:15.367917Z","iopub.status.idle":"2024-02-26T08:53:15.607829Z","shell.execute_reply.started":"2024-02-26T08:53:15.367883Z","shell.execute_reply":"2024-02-26T08:53:15.606620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe, that mostly non-empty images have only one or two ships, and small amount, that have more then 5.","metadata":{}},{"cell_type":"markdown","source":"Now, let's create one more feature \"has_ship\", and plot its distribution, so we can see our class imbalance more obvious. ","metadata":{}},{"cell_type":"code","source":"train_masks_aggregated['has_ship'] = train_masks_aggregated['ship_amount'].map(lambda x: 1 if x > 0 else 0)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:16.182079Z","iopub.execute_input":"2024-02-26T08:53:16.182506Z","iopub.status.idle":"2024-02-26T08:53:16.345741Z","shell.execute_reply.started":"2024-02-26T08:53:16.182473Z","shell.execute_reply":"2024-02-26T08:53:16.344735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ship_counts = train_masks_aggregated['has_ship'].value_counts()\n\ntotal_count = ship_counts.sum()\n\nplt.bar(ship_counts.index, ship_counts.values)\nplt.xlabel('has_ship')\nplt.ylabel('Frequency')\nplt.title('Distribution of Ship Amounts')\n\nfor i, count in enumerate(ship_counts.values):\n    plt.text(i, count, f'{count / total_count * 100:.2f}%', ha='center', va='bottom')\n\nplt.xticks(ship_counts.index, ['No Ship', 'Has Ship'])  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:16.347335Z","iopub.execute_input":"2024-02-26T08:53:16.348353Z","iopub.status.idle":"2024-02-26T08:53:16.549698Z","shell.execute_reply.started":"2024-02-26T08:53:16.348320Z","shell.execute_reply":"2024-02-26T08:53:16.548578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ship_counts","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:16.551614Z","iopub.execute_input":"2024-02-26T08:53:16.551965Z","iopub.status.idle":"2024-02-26T08:53:16.559479Z","shell.execute_reply.started":"2024-02-26T08:53:16.551935Z","shell.execute_reply":"2024-02-26T08:53:16.557987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6, 6))\nplt.pie(ship_counts.values, labels=['No Ship', 'Has Ship'], autopct='%1.2f%%', startangle=140)\nplt.title('Distribution of Ship Amounts')\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:16.821071Z","iopub.execute_input":"2024-02-26T08:53:16.821458Z","iopub.status.idle":"2024-02-26T08:53:16.961433Z","shell.execute_reply.started":"2024-02-26T08:53:16.821427Z","shell.execute_reply":"2024-02-26T08:53:16.960276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can see, how our classes are imbalanced, and i think for this issue, the best strategy will be undersampling of empty images to amount of non-empty images, so our model can distinguish images with ships and without good.","metadata":{}},{"cell_type":"markdown","source":"Now, let's research our image sizes","metadata":{}},{"cell_type":"code","source":"train_masks_aggregated['file_size_kb'] = train_masks_aggregated['ImageId'].map(lambda c_img_id: \n                                                               os.stat(os.path.join(train_dir_path, \n                                                                                    c_img_id)).st_size/1024)\ntrain_masks_aggregated","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:53:17.626952Z","iopub.execute_input":"2024-02-26T08:53:17.627346Z","iopub.status.idle":"2024-02-26T09:05:39.488475Z","shell.execute_reply.started":"2024-02-26T08:53:17.627315Z","shell.execute_reply":"2024-02-26T09:05:39.487206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_masks_aggregated['file_size_kb'].hist()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:39.490954Z","iopub.execute_input":"2024-02-26T09:05:39.491453Z","iopub.status.idle":"2024-02-26T09:05:39.762891Z","shell.execute_reply.started":"2024-02-26T09:05:39.491410Z","shell.execute_reply":"2024-02-26T09:05:39.761640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_masks_aggregated[train_masks_aggregated['file_size_kb']<70]","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:39.764185Z","iopub.execute_input":"2024-02-26T09:05:39.764514Z","iopub.status.idle":"2024-02-26T09:05:39.784658Z","shell.execute_reply.started":"2024-02-26T09:05:39.764487Z","shell.execute_reply":"2024-02-26T09:05:39.783211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_img('0080bd6a5.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:39.788145Z","iopub.execute_input":"2024-02-26T09:05:39.788619Z","iopub.status.idle":"2024-02-26T09:05:40.165994Z","shell.execute_reply.started":"2024-02-26T09:05:39.788578Z","shell.execute_reply":"2024-02-26T09:05:40.164620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_img('0005d01c8.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:40.167622Z","iopub.execute_input":"2024-02-26T09:05:40.168079Z","iopub.status.idle":"2024-02-26T09:05:40.587663Z","shell.execute_reply.started":"2024-02-26T09:05:40.168036Z","shell.execute_reply":"2024-02-26T09:05:40.586507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_img('001bfb70a.jpg')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:40.589164Z","iopub.execute_input":"2024-02-26T09:05:40.589529Z","iopub.status.idle":"2024-02-26T09:05:40.971055Z","shell.execute_reply.started":"2024-02-26T09:05:40.589499Z","shell.execute_reply":"2024-02-26T09:05:40.969575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe, that images, that have under 40 kb size, are corrupted or don't contain useful information, so we need to drop them before training stage  ","metadata":{}},{"cell_type":"code","source":"train_masks_aggregated = train_masks_aggregated[train_masks_aggregated['file_size_kb']>50]","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:40.972713Z","iopub.execute_input":"2024-02-26T09:05:40.973088Z","iopub.status.idle":"2024-02-26T09:05:40.990520Z","shell.execute_reply.started":"2024-02-26T09:05:40.973056Z","shell.execute_reply":"2024-02-26T09:05:40.988882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's undersample our data","metadata":{}},{"cell_type":"code","source":"minority_class = train_masks_aggregated[train_masks_aggregated['has_ship'] == 1]\nmajority_class = train_masks_aggregated[train_masks_aggregated['has_ship'] == 0]\n\nundersampled_majority_class = resample(majority_class, \n                                      replace=False,  \n                                      n_samples=len(minority_class),  \n                                      random_state=42)  \n\nundersampled_train_masks = pd.concat([minority_class, undersampled_majority_class])\nundersampled_train_masks","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:40.992383Z","iopub.execute_input":"2024-02-26T09:05:40.992777Z","iopub.status.idle":"2024-02-26T09:05:41.044649Z","shell.execute_reply.started":"2024-02-26T09:05:40.992744Z","shell.execute_reply":"2024-02-26T09:05:41.043432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"undersampled_train_masks['has_ship'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:41.046031Z","iopub.execute_input":"2024-02-26T09:05:41.046430Z","iopub.status.idle":"2024-02-26T09:05:41.056996Z","shell.execute_reply.started":"2024-02-26T09:05:41.046399Z","shell.execute_reply":"2024-02-26T09:05:41.055705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now i will take sample 5000 empty images and 5000 images with ships, it will help to see results of different models, and saves time, so i can train only model with best performance on full data","metadata":{}},{"cell_type":"code","source":"EMPTY_AMOUNT, SHIPS_AMOUNT = 1000, 15000","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:41.062079Z","iopub.execute_input":"2024-02-26T09:05:41.062546Z","iopub.status.idle":"2024-02-26T09:05:41.068633Z","shell.execute_reply.started":"2024-02-26T09:05:41.062514Z","shell.execute_reply":"2024-02-26T09:05:41.067352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_undersampled_df = pd.merge(train_masks,undersampled_train_masks )\ninput_undersampled_df","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:41.070199Z","iopub.execute_input":"2024-02-26T09:05:41.070985Z","iopub.status.idle":"2024-02-26T09:05:41.277031Z","shell.execute_reply.started":"2024-02-26T09:05:41.070941Z","shell.execute_reply":"2024-02-26T09:05:41.275854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([input_undersampled_df[input_undersampled_df[\"EncodedPixels\"].isna()].sample(EMPTY_AMOUNT), input_undersampled_df[~input_undersampled_df[\"EncodedPixels\"].isna()].sample(SHIPS_AMOUNT)])\ndf","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:41.278878Z","iopub.execute_input":"2024-02-26T09:05:41.279270Z","iopub.status.idle":"2024-02-26T09:05:41.349477Z","shell.execute_reply.started":"2024-02-26T09:05:41.279238Z","shell.execute_reply":"2024-02-26T09:05:41.348098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df.drop(columns=['has_ship','ship_amount','file_size_kb']).to_csv('prep_data.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T11:21:34.364191Z","iopub.execute_input":"2024-02-26T11:21:34.364659Z","iopub.status.idle":"2024-02-26T11:21:35.031030Z","shell.execute_reply.started":"2024-02-26T11:21:34.364626Z","shell.execute_reply":"2024-02-26T11:21:35.029519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIZE=256","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:41.351100Z","iopub.execute_input":"2024-02-26T09:05:41.352359Z","iopub.status.idle":"2024-02-26T09:05:41.357407Z","shell.execute_reply.started":"2024-02-26T09:05:41.352313Z","shell.execute_reply":"2024-02-26T09:05:41.356042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def crop3x3(img, i):\n    \"\"\"img: np.ndarray - original image 768x768\n       i: int 0-8 - image index from crop: 0 1 2\n                                           3 4 5\n                                           6 7 8\n       returns: image 256x256 \n    \"\"\"\n    return img[(i//3)*SIZE: ((i//3)+1)*SIZE,(i%3)*SIZE: (i%3+1)*SIZE]\n\n\ndef crop3x3_mask(img):\n    \"\"\"Returns crop image, crop index with maximum ships area\"\"\"\n    i = K.argmax((\n        K.sum(crop3x3(img, 0)),\n        K.sum(crop3x3(img, 1)),\n        K.sum(crop3x3(img, 2)),\n        K.sum(crop3x3(img, 3)),\n        K.sum(crop3x3(img, 4)),\n        K.sum(crop3x3(img, 5)),\n        K.sum(crop3x3(img, 6)),\n        K.sum(crop3x3(img, 7)),\n        K.sum(crop3x3(img, 8)),\n    ))\n    return (crop3x3(img, i), i)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:41.358815Z","iopub.execute_input":"2024-02-26T09:05:41.359186Z","iopub.status.idle":"2024-02-26T09:05:41.372409Z","shell.execute_reply.started":"2024-02-26T09:05:41.359116Z","shell.execute_reply":"2024-02-26T09:05:41.371098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n\n    def __init__(self, datapath, batch_size, df_mask: pd.DataFrame, augmentation_dict=None):\n        self.datapath = datapath\n        self.batch_size = batch_size\n        self.df = df_mask.sample(frac=1)\n        self.l = len(self.df) // batch_size\n        self.augmentation = None\n        if augmentation_dict is not None:\n            self.augmentation = tf.keras.preprocessing.image.ImageDataGenerator(\n                **augmentation_dict\n            )\n\n    def __len__(self):\n        return self.l\n\n    def on_epoch_end(self):\n        self.df = self.df.sample(frac=1)\n\n    def __getitem__(self, index):\n        mask = np.empty((self.batch_size, SIZE, SIZE), np.float32)\n        image = np.empty((self.batch_size, SIZE, SIZE, 3), np.float32)\n\n        batch_df = self.df[index * self.batch_size: (index + 1) * self.batch_size]\n\n        for b, _, row in zip(range(self.batch_size), range(len(batch_df)), batch_df.itertuples()):\n            temp = tf.keras.preprocessing.image.load_img(self.datapath + '/' + row.ImageId)\n            temp = tf.keras.preprocessing.image.img_to_array(temp) / 255\n\n            mask[b], i = crop3x3_mask(\n                rle_decode(\n                    row.EncodedPixels\n                )\n            )\n            image[b] = crop3x3(temp, i)\n\n        if self.augmentation is not None:\n            augmented_images = []\n            augmented_masks = []\n            for i in range(self.batch_size):\n                augmented = self.augmentation.flow(np.expand_dims(image[i], axis=0),\n                                                    np.expand_dims(mask[i], axis=0),\n                                                    batch_size=1)\n                augmented_image, augmented_mask = next(augmented)\n                augmented_images.append(augmented_image.squeeze())\n                augmented_masks.append(augmented_mask.squeeze())\n            image = np.array(augmented_images)\n            mask = np.array(augmented_masks)\n\n        return image, mask\n    def show_samples(self, num_samples=5):\n        fig, axes = plt.subplots(num_samples, 2, figsize=(10, num_samples * 5))\n        indices = np.random.randint(0, len(self.df), num_samples)\n        \n        for i, idx in enumerate(indices):\n            row = self.df.iloc[idx]\n            image_path = os.path.join(self.datapath, row['ImageId'])\n            temp = tf.keras.preprocessing.image.load_img(image_path)\n            temp = tf.keras.preprocessing.image.img_to_array(temp) / 255\n\n            mask, _ = crop3x3_mask(rle_decode(row['EncodedPixels']))\n            image = crop3x3(temp, _)\n\n            axes[i, 0].imshow(image)\n            axes[i, 0].set_title(f\"Image {i+1}\")\n            axes[i, 0].axis('off')\n\n            axes[i, 1].imshow(mask, cmap='gray')\n            axes[i, 1].set_title(f\"Mask {i+1}\")\n            axes[i, 1].axis('off')\n\n        plt.tight_layout()\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:41.373844Z","iopub.execute_input":"2024-02-26T09:05:41.374704Z","iopub.status.idle":"2024-02-26T09:05:41.397758Z","shell.execute_reply.started":"2024-02-26T09:05:41.374671Z","shell.execute_reply":"2024-02-26T09:05:41.396526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting data into train/validation set and creating DataGenerators","metadata":{}},{"cell_type":"code","source":"dg_args = dict(featurewise_center = False, \n                  samplewise_center = False,\n                  rotation_range = 90, \n                  width_shift_range = 0.2, \n                  height_shift_range = 0.2, \n                  shear_range = 0.1,\n                  zoom_range = [0.9, 1.25],  \n                  horizontal_flip = True, \n                  vertical_flip = True,\n                  fill_mode = 'reflect',\n                   data_format = 'channels_last')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:41.399317Z","iopub.execute_input":"2024-02-26T09:05:41.399850Z","iopub.status.idle":"2024-02-26T09:05:41.415629Z","shell.execute_reply.started":"2024-02-26T09:05:41.399801Z","shell.execute_reply":"2024-02-26T09:05:41.414301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\ntrain_df, valid_df = train_test_split(df, test_size=0.2)\ntrain = DataGenerator(train_dir_path, batch_size, train_df)\nvalid = DataGenerator(train_dir_path, batch_size, valid_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:41.417264Z","iopub.execute_input":"2024-02-26T09:05:41.417744Z","iopub.status.idle":"2024-02-26T09:05:41.436076Z","shell.execute_reply.started":"2024-02-26T09:05:41.417702Z","shell.execute_reply":"2024-02-26T09:05:41.435032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.show_samples(5)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:41.438050Z","iopub.execute_input":"2024-02-26T09:05:41.438513Z","iopub.status.idle":"2024-02-26T09:05:43.571196Z","shell.execute_reply.started":"2024-02-26T09:05:41.438473Z","shell.execute_reply":"2024-02-26T09:05:43.569840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train), len(valid)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:43.572619Z","iopub.execute_input":"2024-02-26T09:05:43.573073Z","iopub.status.idle":"2024-02-26T09:05:43.580854Z","shell.execute_reply.started":"2024-02-26T09:05:43.573041Z","shell.execute_reply":"2024-02-26T09:05:43.579621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.show_samples(5)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:43.582315Z","iopub.execute_input":"2024-02-26T09:05:43.582671Z","iopub.status.idle":"2024-02-26T09:05:45.632561Z","shell.execute_reply.started":"2024-02-26T09:05:43.582642Z","shell.execute_reply":"2024-02-26T09:05:45.631356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Losses","metadata":{}},{"cell_type":"code","source":"def IoU(y_true, y_pred, eps=1e-6):\n    if K.max(y_true) == 0.0:\n        return IoU(1-y_true, 1-y_pred) ## empty image; calc IoU of zeros\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection\n    return -K.mean( (intersection + eps) / (union + eps), axis=0)\n\ndef dice_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n\ndef combo_loss(y_true, y_pred):\n    return 1e-3 * binary_crossentropy(y_true, y_pred) - dice_coef(y_true, y_pred)\n\ndef jaccard_index(y_true, y_pred, smooth=1):\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection\n    return -K.mean((intersection + smooth) / (union + smooth), axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:45.633874Z","iopub.execute_input":"2024-02-26T09:05:45.634317Z","iopub.status.idle":"2024-02-26T09:05:45.653429Z","shell.execute_reply.started":"2024-02-26T09:05:45.634288Z","shell.execute_reply":"2024-02-26T09:05:45.651775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def BCE_dice(y_true, y_pred):\n    return  K.binary_crossentropy(y_true, y_pred)+  (1-dice_score(y_true, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:45.655636Z","iopub.execute_input":"2024-02-26T09:05:45.656806Z","iopub.status.idle":"2024-02-26T09:05:45.676178Z","shell.execute_reply.started":"2024-02-26T09:05:45.656756Z","shell.execute_reply":"2024-02-26T09:05:45.674838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_score(y_true, y_pred):\n    return (2.0*K.sum(y_pred * y_true)+0.0001) / (K.sum(y_true)+ K.sum(y_pred)+0.0001)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:45.678117Z","iopub.execute_input":"2024-02-26T09:05:45.679159Z","iopub.status.idle":"2024-02-26T09:05:45.692249Z","shell.execute_reply.started":"2024-02-26T09:05:45.679079Z","shell.execute_reply":"2024-02-26T09:05:45.690358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline model \n","metadata":{}},{"cell_type":"code","source":" \ndef encoder_block(inputs, num_filters): \n  \n    # Convolution with 3x3 filter followed by ReLU activation \n    x = tf.keras.layers.Conv2D(num_filters,  \n                               3,  \n                               padding = 'same')(inputs) \n    x = tf.keras.layers.Activation('elu')(x) \n      \n    # Convolution with 3x3 filter followed by ReLU activation \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Conv2D(num_filters,  \n                               3,  \n                               padding = 'same')(x) \n    x = tf.keras.layers.Activation('elu')(x) \n  \n    # Max Pooling with 2x2 filter \n    x = tf.keras.layers.MaxPool2D(pool_size = (2, 2), \n                                  strides = 2)(x) \n      \n    return x","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:45.694296Z","iopub.execute_input":"2024-02-26T09:05:45.694898Z","iopub.status.idle":"2024-02-26T09:05:45.707763Z","shell.execute_reply.started":"2024-02-26T09:05:45.694866Z","shell.execute_reply":"2024-02-26T09:05:45.706314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decoder_block(inputs, skip_features, num_filters): \n  \n    # Upsampling with 2x2 filter \n    x = tf.keras.layers.Conv2DTranspose(num_filters, \n                                        (2, 2),  \n                                        strides = 2,  \n                                        padding = 'same')(inputs) \n      \n    # Copy and crop the skip features  \n    # to match the shape of the upsampled input \n    skip_features = tf.image.resize(skip_features, \n                                    size = (x.shape[1], \n                                            x.shape[2])) \n    x = tf.keras.layers.Concatenate()([x, skip_features]) \n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n\n    # Convolution with 3x3 filter followed by ReLU activation \n    x = tf.keras.layers.Conv2D(num_filters, \n                               3,  \n                               padding = 'same')(x) \n    x = tf.keras.layers.Activation('elu')(x) \n    \n    # Convolution with 3x3 filter followed by ReLU activation \n    x = tf.keras.layers.Conv2D(num_filters, 3, padding = 'same')(x) \n    x = tf.keras.layers.Activation('elu')(x) \n      \n    return x","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:45.709555Z","iopub.execute_input":"2024-02-26T09:05:45.709922Z","iopub.status.idle":"2024-02-26T09:05:45.721143Z","shell.execute_reply.started":"2024-02-26T09:05:45.709893Z","shell.execute_reply":"2024-02-26T09:05:45.719816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unet_model(input_shape = (SIZE, SIZE, 3), num_classes = 1): \n    inputs = tf.keras.layers.Input(input_shape) \n      \n    # Contracting Path \n    s1 = encoder_block(inputs, 16) \n    s2 = encoder_block(s1, 32) \n    s3 = encoder_block(s2, 64) \n    s4 = encoder_block(s3, 128) \n      \n    # Bottleneck \n    b1 = tf.keras.layers.Conv2D(128, 3, padding = 'same')(s4) \n    b1 = tf.keras.layers.Activation('elu')(b1) \n    b1 = tf.keras.layers.Dropout(0.2)(b1)\n    b1 = tf.keras.layers.Conv2D(128, 3, padding = 'same')(b1) \n    b1 = tf.keras.layers.Activation('elu')(b1) \n      \n    # Expansive Path \n    s5 = decoder_block(b1, s4, 128) \n    s6 = decoder_block(s5, s3, 64) \n    s7 = decoder_block(s6, s2, 32) \n    s8 = decoder_block(s7, s1, 16) \n      \n    # Output \n    outputs = tf.keras.layers.Conv2D(num_classes,  \n                                     1,  \n                                     padding = 'same',  \n                                     activation = 'sigmoid')(s8) \n      \n    model = tf.keras.models.Model(inputs = inputs,  \n                                  outputs = outputs,  \n                                  name = 'U-Net') \n    return model ","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:45.722886Z","iopub.execute_input":"2024-02-26T09:05:45.723304Z","iopub.status.idle":"2024-02-26T09:05:45.740188Z","shell.execute_reply.started":"2024-02-26T09:05:45.723272Z","shell.execute_reply":"2024-02-26T09:05:45.738897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = unet_model(num_classes=1) \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:45.741609Z","iopub.execute_input":"2024-02-26T09:05:45.742253Z","iopub.status.idle":"2024-02-26T09:05:46.528314Z","shell.execute_reply.started":"2024-02-26T09:05:45.742213Z","shell.execute_reply":"2024-02-26T09:05:46.527137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss=BCE_dice, metrics=[dice_score])","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:05:46.533546Z","iopub.execute_input":"2024-02-26T09:05:46.533925Z","iopub.status.idle":"2024-02-26T09:05:46.560272Z","shell.execute_reply.started":"2024-02-26T09:05:46.533895Z","shell.execute_reply":"2024-02-26T09:05:46.558729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dir = '/kaggle/working/models'","metadata":{"execution":{"iopub.status.busy":"2024-02-24T18:21:56.304646Z","iopub.execute_input":"2024-02-24T18:21:56.305004Z","iopub.status.idle":"2024-02-24T18:21:56.311395Z","shell.execute_reply.started":"2024-02-24T18:21:56.304974Z","shell.execute_reply":"2024-02-24T18:21:56.310349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n\nif not os.path.isdir(model_dir):\n    os.mkdir(model_dir)\n    \nweight_path=\"{}_weights.best.hdf5\".format(model_dir)\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.33, \n                                   patience=1, \n                                   verbose=1, mode='min', min_delta=0.0001, cooldown=0, min_lr=1e-8)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\",\n                      verbose = 2,\n                      patience=10) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","metadata":{"execution":{"iopub.status.busy":"2024-02-24T18:21:56.351720Z","iopub.execute_input":"2024-02-24T18:21:56.352056Z","iopub.status.idle":"2024-02-24T18:21:56.359973Z","shell.execute_reply.started":"2024-02-24T18:21:56.352028Z","shell.execute_reply":"2024-02-24T18:21:56.358943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_history = model.fit(train,\n                             epochs=10, \n                             validation_data=valid,\n                             callbacks=callbacks_list,\n                             batch_size=16\n                                       )","metadata":{"execution":{"iopub.status.busy":"2024-02-24T19:39:14.283208Z","iopub.execute_input":"2024-02-24T19:39:14.283636Z","iopub.status.idle":"2024-02-24T20:54:05.320129Z","shell.execute_reply.started":"2024-02-24T19:39:14.283606Z","shell.execute_reply":"2024-02-24T20:54:05.319141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('/kaggle/input/saved-model/models_weights.best (1).hdf5')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:06:57.091135Z","iopub.execute_input":"2024-02-26T09:06:57.091543Z","iopub.status.idle":"2024-02-26T09:06:57.278859Z","shell.execute_reply.started":"2024-02-26T09:06:57.091513Z","shell.execute_reply":"2024-02-26T09:06:57.277099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_y = model.predict(valid)\nprint(pred_y.shape, pred_y.min(), pred_y.max(), pred_y.mean())","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:07:14.182816Z","iopub.execute_input":"2024-02-26T09:07:14.183802Z","iopub.status.idle":"2024-02-26T09:12:37.941902Z","shell.execute_reply.started":"2024-02-26T09:07:14.183762Z","shell.execute_reply":"2024-02-26T09:12:37.940548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_paths = os.listdir(test_dir_path)\nprint(len(test_paths), 'test images found')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:48:29.376714Z","iopub.execute_input":"2024-02-26T09:48:29.377141Z","iopub.status.idle":"2024-02-26T09:48:29.390441Z","shell.execute_reply.started":"2024-02-26T09:48:29.377088Z","shell.execute_reply":"2024-02-26T09:48:29.389205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SCALING = (3,3)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:20:45.533799Z","iopub.execute_input":"2024-02-26T09:20:45.534566Z","iopub.status.idle":"2024-02-26T09:20:45.539385Z","shell.execute_reply.started":"2024-02-26T09:20:45.534515Z","shell.execute_reply":"2024-02-26T09:20:45.538217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(img_name):\n    c_path = os.path.join(test_dir_path, img_name)\n    c_img = imread(c_path)\n    img = np.expand_dims(c_img, 0)/255.0\n#     img = crop\n    if IMG_SCALING is not None:\n        img = img[:, ::IMG_SCALING[0], ::IMG_SCALING[1]]\n    return img, model.predict(img, verbose=0) ","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:49:00.195679Z","iopub.execute_input":"2024-02-26T09:49:00.196528Z","iopub.status.idle":"2024-02-26T09:49:00.203532Z","shell.execute_reply.started":"2024-02-26T09:49:00.196487Z","shell.execute_reply":"2024-02-26T09:49:00.202349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage.io import imread","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:49:01.024170Z","iopub.execute_input":"2024-02-26T09:49:01.024983Z","iopub.status.idle":"2024-02-26T09:49:01.029184Z","shell.execute_reply.started":"2024-02-26T09:49:01.024947Z","shell.execute_reply":"2024-02-26T09:49:01.028359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, m_axs = plt.subplots(20, 2, figsize = (10, 40))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\nfor (ax1, ax2), c_img_name in zip(m_axs, test_paths):\n    img, pred = predict(c_img_name)\n    ax1.imshow(img[0])\n    ax1.set_title('Image')\n    ax2.imshow(pred[0, :, :, 0], vmin = 0, vmax = 1)\n    ax2.set_title('Prediction')\nfig.savefig('test_predictions.png')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:49:01.459908Z","iopub.execute_input":"2024-02-26T09:49:01.460855Z","iopub.status.idle":"2024-02-26T09:49:09.681782Z","shell.execute_reply.started":"2024-02-26T09:49:01.460818Z","shell.execute_reply":"2024-02-26T09:49:09.680563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import ndimage\n\ndef split_mask(mask):\n    threshold = 0.6\n    threshold_obj = 8 #ignor predictions composed of \"threshold_obj\" pixels or less\n    labeled,n_objs = ndimage.label(mask > threshold)\n    result = []\n    for i in range(n_objs):\n        obj = (labeled == i + 1).astype(int)\n        if(obj.sum() > threshold_obj): result.append(obj)\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:49:09.683924Z","iopub.execute_input":"2024-02-26T09:49:09.684313Z","iopub.status.idle":"2024-02-26T09:49:09.692096Z","shell.execute_reply.started":"2024-02-26T09:49:09.684281Z","shell.execute_reply":"2024-02-26T09:49:09.691011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_encode(img):\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:49:09.693807Z","iopub.execute_input":"2024-02-26T09:49:09.694445Z","iopub.status.idle":"2024-02-26T09:49:09.704781Z","shell.execute_reply.started":"2024-02-26T09:49:09.694406Z","shell.execute_reply":"2024-02-26T09:49:09.703491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_run_length_encoded_predictions(y_pred, img_name):\n    list_dict = []\n    masks = split_mask(y_pred)\n#     masks = multi_rle_encode(y_pred)\n    if len(masks) == 0:\n        list_dict.append({\"ImageId\": img_name, \"EncodedPixels\": np.nan})\n    for mask in masks:\n        list_dict.append({\"ImageId\": img_name, \"EncodedPixels\": rle_encode(mask)})\n    return list_dict","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:49:09.707206Z","iopub.execute_input":"2024-02-26T09:49:09.708051Z","iopub.status.idle":"2024-02-26T09:49:09.721787Z","shell.execute_reply.started":"2024-02-26T09:49:09.708009Z","shell.execute_reply":"2024-02-26T09:49:09.720430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_and_decode(test_img_names):\n    list_dict = []\n    for img_name in test_img_names:\n        _ , pred = predict(img_name)\n        rle_pred = get_run_length_encoded_predictions(pred[0], img_name)\n        list_dict += rle_pred\n    return pd.DataFrame(list_dict, columns=[\"ImageId\", \"EncodedPixels\"])    ","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:49:09.723586Z","iopub.execute_input":"2024-02-26T09:49:09.724072Z","iopub.status.idle":"2024-02-26T09:49:09.736089Z","shell.execute_reply.started":"2024-02-26T09:49:09.724038Z","shell.execute_reply":"2024-02-26T09:49:09.734635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_decoded_pred_df = predict_and_decode(test_paths)\ntest_decoded_pred_df.to_csv('submission.csv', index=False)\ntest_decoded_pred_df","metadata":{"execution":{"iopub.status.busy":"2024-02-26T09:49:09.737645Z","iopub.execute_input":"2024-02-26T09:49:09.738061Z","iopub.status.idle":"2024-02-26T10:37:57.877225Z","shell.execute_reply.started":"2024-02-26T09:49:09.738022Z","shell.execute_reply":"2024-02-26T10:37:57.875852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import zipfile\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-02-26T11:29:42.198406Z","iopub.execute_input":"2024-02-26T11:29:42.199144Z","iopub.status.idle":"2024-02-26T11:29:42.204549Z","shell.execute_reply.started":"2024-02-26T11:29:42.199096Z","shell.execute_reply":"2024-02-26T11:29:42.203187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\n# Disable displaying warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T11:35:46.152780Z","iopub.execute_input":"2024-02-26T11:35:46.155536Z","iopub.status.idle":"2024-02-26T11:35:46.163168Z","shell.execute_reply.started":"2024-02-26T11:35:46.155478Z","shell.execute_reply":"2024-02-26T11:35:46.162048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with zipfile.ZipFile('prep_images.zip', 'w') as zip:\n    for id in df.ImageId:\n        img_path = os.path.join(train_dir_path, id)\n        img = Image.open(img_path)\n            # Add the image file to the Zip file\n        zip.write(img_path, os.path.basename(img_path))","metadata":{"execution":{"iopub.status.busy":"2024-02-26T11:35:46.434467Z","iopub.execute_input":"2024-02-26T11:35:46.435701Z","iopub.status.idle":"2024-02-26T11:37:05.509754Z","shell.execute_reply.started":"2024-02-26T11:35:46.435658Z","shell.execute_reply":"2024-02-26T11:37:05.508429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}